import ollama
from typing import List, Optional, Generator
from PIL import Image
import io
import json
import sys

class Think:

    def generate_response(
        self,
        prompt: str,
        model_name: str,
        provider: str,
        image: Optional[Image.Image] = None,
        history: Optional[List[dict]] = None,
        system_message: Optional[str] = None,
        depth: int = 1
    ) -> Generator[str, None, None]:
        """
        Generates a response using the specified Ollama model, iterating 'depth' times,
        and returning blocks of text iteratively.

        Args:
            prompt (str): Input prompt.
            model_name (str): Name of the Ollama model to use.
            provider (str): Name of the provider.
            image (Optional[Image.Image]): Optional image that can be included in the prompt.
            history (Optional[List[dict]]): Previous message history.
            system_message (Optional[str]): System message that can be included in the prompt.
            depth (int): Number of iterations.

        Yields:
            str: Fragments of the response generated by Ollama.
        """
        try:
            messages = []
            if system_message:
                messages.append({"role": "system", "content": system_message})
            if history:
                for message in history:
                    if message["role"] == "model":
                        messages.append({"role": "assistant", "content": message["content"]})
                    else:
                        messages.append({"role": message["role"], "content": message["content"]})

            if image:
                # Convert PIL image to bytes
                image_bytes = io.BytesIO()
                image.save(image_bytes, format=image.format if image.format else "PNG")
                image_bytes = image_bytes.getvalue()

                messages.append({"role": "user", "content": prompt, "images": [image_bytes]})
            else:
                messages.append({"role": "user", "content": prompt})

            # Ensure we only use the model name (separated by :)
            model_name = model_name.split(":")[0]

            final_prompt = prompt
            full_response = ""

            for i in range(depth):
                print(f"\n--- Iteration #{i + 1} of {depth} ---")

                # Clear the message history before each iteration
                messages = []
                if system_message:
                    messages.append({"role": "system", "content": system_message})
                if history:
                    for message in history:
                        if message["role"] == "model":
                            messages.append({"role": "assistant", "content": message["content"]})
                        else:
                            messages.append({"role": message["role"], "content": message["content"]})
                messages.append({"role": "user", "content": final_prompt})

                # Use the provider to call the model
                from llm import llm_providers
                llm_provider = llm_providers.get(provider)
                if llm_provider:
                    response_generator = llm_provider.generate_response(prompt=final_prompt, model_name=model_name, image=image, history=history, system_message=system_message)
                    current_response = ""
                    for chunk in response_generator:
                        current_response += chunk
                        yield chunk
                    full_response += current_response
                    if i < depth - 1:
                        final_prompt = full_response + "\nWait, I have to think it again..."
                else:
                    yield f"Error: Provider {provider} not found."

        except Exception as e:
            yield f"Error generating response: {e}"


