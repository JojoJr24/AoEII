from utils import BLUE, GREEN, MAGENTA, debug_print
from providers import llm_providers
from tool_manager import load_tools, generate_tool_descriptions, parse_tool_calls, execute_tools
import json
import os
from db import save_simple_response

def generate_response(prompt, model_name, image=None, history=None, provider_name=None, system_message=None, selected_tools=None, base_url=None):
    """
    Generate a response using the specified LLM provider and model, with optional tool integration.

    Args:
        prompt (str): The input prompt for the LLM.
        model_name (str): The name of the model to use.
        image (Optional): Optional image input for multimodal models.
        history (Optional): Conversation history to provide context.
        provider_name (str): The name of the LLM provider.
        system_message (Optional): Optional system message for context.
        selected_tools (list): List of tools to use.
        base_url (Optional): Base URL for API requests.

    Returns:
        str: The response generated by the LLM or error message.
    """
    debug_print(BLUE, f"Generating response with provider: {provider_name}, model: {model_name}, tools: {selected_tools}")

    provider = llm_providers.get(provider_name)
    if not provider:
        debug_print(MAGENTA, "Error: LLM provider not found")
        return "Error: LLM provider not found"

    if not model_name:
        debug_print(MAGENTA, "Error: No model selected for the provider.")
        return "Error: No model selected for the provider."

    tool_instances = []
    if selected_tools:
        tool_instances = load_tools(selected_tools)

    tool_descriptions = generate_tool_descriptions(tool_instances)

    if tool_instances:
        tool_response, prompt = process_tools_with_llm(
            provider, model_name, prompt, tool_descriptions, system_message, tool_instances
        )

    response = provider.generate_response(prompt=prompt, model_name=model_name, image=image, history=history, system_message=system_message)
    debug_print(GREEN, "Response generated successfully.")
    return response

def process_tools_with_llm(provider, model_name, prompt, tool_descriptions, system_message, tool_instances):
    """
    Use the LLM to generate tool calls and process their results.

    Args:
        provider: The LLM provider instance.
        model_name (str): The model to use.
        prompt (str): The input prompt for the LLM.
        tool_descriptions (str): Descriptions of the available tools.
        system_message (str): Optional system message.
        tool_instances (list): List of tool instances.

    Returns:
        tuple: Updated tool response and prompt.
    """
    tool_prompt = f"""
        You have access to the following tools. Use one or more as needed:
        {tool_descriptions}

        Use the following format to call tools:

        tool_code
        [
            {{
                "tool_name": "tool_name_1",
                "parameters": {{ "param1": "value1" }}
            }},
            {{
                "tool_name": "tool_name_2",
                "parameters": {{ "param2": "value2" }}
            }}
        ]

        Now, respond to the following prompt:
        {prompt}
    """
    tool_response_generator = provider.generate_response(tool_prompt, model_name, None, None, system_message)
    tool_response = "".join(tool_response_generator)
    debug_print(MAGENTA,tool_response)

    try:
        tool_calls = parse_tool_calls(tool_response)
        tool_results = execute_tools(tool_calls, tool_instances)

        tool_results_str = json.dumps(tool_results, indent=4)
        prompt = f"""
            The following tools were called:
            {tool_results_str}

            Now, respond to the following prompt:
            {prompt}
        """
    except json.JSONDecodeError:
        debug_print(MAGENTA, "Error decoding tool response.")
        prompt = f"Error decoding tool response. Now, respond to the following prompt: {prompt}"

    return tool_response, prompt

def generate_simple_response(prompt):
    """
    Generates a simplified response using Ollama with Phi4 model and all available tools.
    
    Args:
        prompt (str): The input prompt.
    
    Returns:
        str: The generated response.
    """
    tools_dir = '../tools'
    tools = []
    for filename in os.listdir(tools_dir):
        if filename.endswith('.py'):
            tools.append(os.path.splitext(filename)[0])
    
    model_name = os.getenv("SIMPLE_RESPONSE_MODEL", "models/gemini-2.0-flash-exp")
    provider_name = os.getenv("SIMPLE_RESPONSE_PROVIDER", "gemini")

    response = ""
    for chunk in generate_response(
        prompt=prompt,
        model_name=model_name,
        provider_name=provider_name,
        selected_modes=tools,
        history=None,
        system_message=None
    ):
        response += chunk
    save_simple_response(prompt, response)
    return response