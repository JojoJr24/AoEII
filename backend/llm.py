import time
from claude_api import ClaudeAPI
from gemini_api import GeminiAPI
from ollama_api import OllamaAPI
from utils import GREEN, RED, BLUE, MAGENTA, RESET
from openai_api import OpenAIAPI
from groq_api import GroqAPI
from typing import Generator
import importlib.util
import os
import json

from ..think.think import Think
from utils import GREEN, debug_print

# Initialize the Gemini API
gemini_api = GeminiAPI()
debug_print(BLUE, "Gemini API initialized.")

# Initialize the Ollama API
try:
    ollama_api = OllamaAPI()
    debug_print(BLUE, "Ollama API initialized.")
except Exception as e:
    debug_print(MAGENTA, f"Error initializing Ollama API: {e}")
    ollama_api = None

# Initialize the OpenAI API
try:
    openai_api = OpenAIAPI()
    debug_print(BLUE, "OpenAI API initialized.")
except Exception as e:
    try:
        openai_compatible_api = OpenAIAPI(base_url=os.getenv("OPENAI_COMPATIBLE_BASE_URL"))
        debug_print(BLUE, "OpenAI Compatible API initialized.")
    except Exception as e:
        debug_print(MAGENTA, f"Error initializing OpenAI API: {e}")
        openai_api = None
        openai_compatible_api = None
    else:
        openai_api = openai_compatible_api

# Initialize the Claude API
try:
    claude_api = ClaudeAPI()
    debug_print(BLUE, "Claude API initialized.")
except Exception as e:
    debug_print(MAGENTA, f"Error initializing Claude API: {e}")
    claude_api = None

# Initialize the Groq API
groq_api = None
try:
    groq_api = GroqAPI()
    debug_print(BLUE, "Groq API initialized.")
except Exception as e:
    debug_print(MAGENTA, f"Error initializing Groq API: {e}")
    groq_api = None

# Initialize Think class
think_instance = Think()

def think(prompt: str, depth: int, selected_model=None, selected_provider=None) -> Generator[str, None, None]:
    """
    Invokes the generate_response method from the Think class and returns the response.
    """
    if not selected_model:
        selected_model = selected_model

    # provider = llm_providers.get(selected_provider)
    # if not provider:
    #     yield "Error: Provider not found"
    #     return

    for chunk in think_instance.generate_response(
        prompt=prompt,
        model_name=selected_model,
        depth=depth
    ):
        yield chunk

# Dictionary to hold available LLM providers
llm_providers = {
    "gemini": gemini_api,
}

if ollama_api:
    llm_providers["ollama"] = ollama_api
else:
    debug_print(True, "Ollama API not available, setting empty model list.")

if openai_api:
    llm_providers["openai"] = openai_api
    llm_providers["openai_compatible"] = openai_api
else:
    debug_print(True, "OpenAI API not available, setting empty model list.")

if claude_api:
    llm_providers["claude"] = claude_api
else:
    debug_print(True, "Claude API not available, setting empty model list.")

if groq_api:
    llm_providers["groq"] = groq_api
else:
    debug_print(True, "Groq API not available, setting empty model list.")

# Default LLM provider and model
selected_provider = "gemini"
selected_model = "gemini-1.5-flash"



# Function to generate responses using the selected LLM
def generate_response(prompt, model_name, image=None, history=None, provider_name=None, system_message=None, selected_tools=None, base_url=None):
    """
    Generate a response using the specified LLM provider and model, with optional tool integration.

    Args:
        prompt (str): The input prompt for the LLM.
        model_name (str): The name of the model to use.
        image (Optional): Optional image input for multimodal models.
        history (Optional): Conversation history to provide context.
        provider_name (str): The name of the LLM provider.
        system_message (Optional): Optional system message for context.
        selected_tools (list): List of tools to use.
        base_url (Optional): Base URL for API requests.

    Returns:
        str: The response generated by the LLM or error message.
    """
    debug_print(BLUE, f"Generating response with provider: {provider_name}, model: {model_name}, tools: {selected_tools}")

    provider = llm_providers.get(provider_name)
    if not provider:
        debug_print(MAGENTA, "Error: LLM provider not found")
        return "Error: LLM provider not found"

    if not model_name:
        debug_print(MAGENTA, "Error: No model selected for the provider.")
        return "Error: No model selected for the provider."

    tool_instances = []
    if selected_tools:
        tool_instances = load_tools(selected_tools)

    tool_descriptions = generate_tool_descriptions(tool_instances)

    if tool_instances:
        tool_response, prompt = process_tools_with_llm(
            provider, model_name, prompt, tool_descriptions, system_message, tool_instances
        )

    response = provider.generate_response(prompt, model_name, image, history, system_message)
    debug_print(GREEN, "Response generated successfully.")
    return response

def load_tools(selected_modes):
    """
    Load tool modules and validate their structure.

    Args:
        selected_modes (list): List of tool modes to load.

    Returns:
        list: List of tool instances with execute and description methods.
    """
    tools_dir = '../tools'
    tool_instances = []
    from tools import list_tools_by_mode
    for mode in selected_modes:
        tool_names = list_tools_by_mode(mode)
        for tool_name in tool_names:
            try:
                file_path = os.path.join(tools_dir, f'{tool_name}.py')
                spec = importlib.util.spec_from_file_location(tool_name, file_path)
                module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(module)
                if hasattr(module, 'execute') and hasattr(module, 'get_tool_description'):
                    tool_instances.append({
                            'name': tool_name,
                            'description': module.get_tool_description(),
                            'execute': module.execute
                        })
                else:
                    debug_print(MAGENTA, f"Error: Tool {tool_name} lacks required methods.")
            except Exception as e:
                debug_print(MAGENTA, f"Error loading tool {tool_name}: {e}")

    return tool_instances

def generate_tool_descriptions(tool_instances):
    """
    Generate a formatted string of tool descriptions for use in prompts.

    Args:
        tool_instances (list): List of tool instances.

    Returns:
        str: Formatted tool descriptions.
    """
    return "\n".join([f"- {tool['name']}: {tool['description']}" for tool in tool_instances])

def process_tools_with_llm(provider, model_name, prompt, tool_descriptions, system_message, tool_instances):
    """
    Use the LLM to generate tool calls and process their results.

    Args:
        provider: The LLM provider instance.
        model_name (str): The model to use.
        prompt (str): The input prompt for the LLM.
        tool_descriptions (str): Descriptions of the available tools.
        system_message (str): Optional system message.
        tool_instances (list): List of tool instances.

    Returns:
        tuple: Updated tool response and prompt.
    """
    tool_prompt = f"""
        You have access to the following tools. Use one or more as needed:
        {tool_descriptions}

        Use the following format to call tools:

        tool_code
        [
            {{
                "tool_name": "tool_name_1",
                "parameters": {{ "param1": "value1" }}
            }},
            {{
                "tool_name": "tool_name_2",
                "parameters": {{ "param2": "value2" }}
            }}
        ]

        Now, respond to the following prompt:
        {prompt}
    """

    tool_response_generator = provider.generate_response(tool_prompt, model_name, None, None, system_message)
    tool_response = "".join(tool_response_generator)

    try:
        tool_calls = parse_tool_calls(tool_response)
        tool_results = execute_tools(tool_calls, tool_instances)

        tool_results_str = json.dumps(tool_results, indent=4)
        prompt = f"""
            The following tools were called:
            {tool_results_str}

            Now, respond to the following prompt:
            {prompt}
        """
    except json.JSONDecodeError:
        debug_print(MAGENTA, "Error decoding tool response.")
        prompt = f"Error decoding tool response. Now, respond to the following prompt: {prompt}"

    return tool_response, prompt

def parse_tool_calls(tool_response):
    """
    Parse tool calls from the LLM's response.

    Args:
        tool_response (str): The raw response from the LLM containing tool calls.

    Returns:
        list: List of parsed tool calls.
    """
    start_index = tool_response.find('[')
    end_index = tool_response.rfind(']')

    if start_index == -1 or end_index == -1:
        raise json.JSONDecodeError("No tool calls found", tool_response, 0)

    json_string = tool_response[start_index:end_index + 1]
    return json.loads(json_string)

def execute_tools(tool_calls, tool_instances):
    """
    Execute the specified tools and collect results.

    Args:
        tool_calls (list): List of tool calls to execute.
        tool_instances (list): List of available tool instances.

    Returns:
        list: Results of executed tools.
    """
    results = []

    for call in tool_calls:
        tool_name = call.get('tool_name')
        params = call.get('parameters', {})
        tool = next((t for t in tool_instances if t['name'] == tool_name), None)

        if tool:
            try:
                debug_print(BLUE, f"Executing tool: {tool_name} with params: {params}")
                result = tool['execute'](**params)
                debug_print(GREEN, f"Tool result: {result}")
                results.append({"tool_name": tool_name, "tool_params": params, "tool_result": result})
            except Exception as e:
                debug_print(MAGENTA, f"Error executing tool {tool_name}: {e}")
                results.append({"tool_name": tool_name, "error": str(e)})
        else:
            debug_print(MAGENTA, f"Error: Tool {tool_name} not found.")
            results.append({"tool_name": tool_name, "error": "Tool not found"})

    return results


def generate_think_response(prompt, depth, model_name=None, provider_name=None):
    if not model_name:
        model_name = selected_model
    if not provider_name:
        provider_name = selected_provider
    debug_print(BLUE, f"Generating think response with model: {model_name}, provider: {provider_name}, depth: {depth}")
    
    response = think(prompt, depth, selected_model=model_name, selected_provider=provider_name)
    debug_print(GREEN, f"Think response generated successfully.")
    return response

def generate_simple_response(prompt):
    """
    Generates a simplified response using Ollama with Phi4 model and all available tools.
    
    Args:
        prompt (str): The input prompt.
    
    Returns:
        str: The generated response.
    """
    tools_dir = '../tools'
    tools = []
    for filename in os.listdir(tools_dir):
        if filename.endswith('.py'):
            tools.append(os.path.splitext(filename)[0])
    
    model_name = os.getenv("SIMPLE_RESPONSE_MODEL", "models/gemini-2.0-flash-exp")
    provider_name = os.getenv("SIMPLE_RESPONSE_PROVIDER", "gemini")

    response = ""
    for chunk in generate_response(
        prompt=prompt,
        model_name=model_name,
        provider_name=provider_name,
        selected_modes=tools,
        history=None,
        system_message=None
    ):
        response += chunk
    from db import save_simple_response
    save_simple_response(prompt, response)
    return response
